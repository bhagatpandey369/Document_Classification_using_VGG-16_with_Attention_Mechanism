{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e03fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import copy\n",
    "import cv2\n",
    "import csv\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as utils\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16_bn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d96a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/rishab/alexnet_attention/train\"\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = datasets.ImageFolder(root=root_dir,transform=transform)\n",
    "\n",
    "train_size = 0.8 \n",
    "train_data, val_data = train_test_split(dataset, train_size=train_size, shuffle=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9da18c-fed1-4b67-8271-5e1f3be4cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.up_factor = up_factor\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
    "    def forward(self, l, g):\n",
    "        N, C, W, H = l.size()\n",
    "        l_ = self.W_l(l)\n",
    "        g_ = self.W_g(g)\n",
    "        if self.up_factor > 1:\n",
    "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
    "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
    "        \n",
    "        # compute attn map\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        # re-weight the local feature\n",
    "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
    "        if self.normalize_attn:\n",
    "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
    "        else:\n",
    "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n",
    "        return a, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102c060d-833f-4837-b979-361c52004fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnVGG(nn.Module):\n",
    "    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n",
    "        super(AttnVGG, self).__init__()\n",
    "        net = models.vgg16_bn(weights=torchvision.models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n",
    "        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n",
    "        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n",
    "        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n",
    "        self.pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dpt = None\n",
    "        if dropout is not None:\n",
    "            self.dpt = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)\n",
    "        \n",
    "       # initialize the attention blocks defined above\n",
    "        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)\n",
    "        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)\n",
    "        \n",
    "       \n",
    "        self.reset_parameters(self.cls)\n",
    "        self.reset_parameters(self.attn1)\n",
    "        self.reset_parameters(self.attn2)\n",
    "    def reset_parameters(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0., 0.01)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1(x)       # /1\n",
    "        pool1 = F.max_pool2d(block1, 2, 2) # /2\n",
    "        block2 = self.conv_block2(pool1)   # /2\n",
    "        pool2 = F.max_pool2d(block2, 2, 2) # /4\n",
    "        block3 = self.conv_block3(pool2)   # /4\n",
    "        pool3 = F.max_pool2d(block3, 2, 2) # /8\n",
    "        block4 = self.conv_block4(pool3)   # /8\n",
    "        pool4 = F.max_pool2d(block4, 2, 2) # /16\n",
    "        block5 = self.conv_block5(pool4)   # /16\n",
    "        pool5 = F.max_pool2d(block5, 2, 2) # /32\n",
    "        N, __, __, __ = pool5.size()\n",
    "        \n",
    "        g = self.pool(pool5).view(N,512)\n",
    "        a1, g1 = self.attn1(pool3, pool5)\n",
    "        a2, g2 = self.attn2(pool4, pool5)\n",
    "        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n",
    "        if self.dpt is not None:\n",
    "            g_hat = self.dpt(g_hat)\n",
    "        out = self.cls(g_hat)\n",
    "\n",
    "        return [out, a1, a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899e63b0-5e2b-4a29-9195-55f4c80bb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttnVGG(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_block4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv_block5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (cls): Linear(in_features=1280, out_features=16, bias=True)\n",
      "  (attn1): AttentionBlock(\n",
      "    (W_l): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_g): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (phi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (attn2): AttentionBlock(\n",
      "    (W_l): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (W_g): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (phi): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AttnVGG(num_classes = 16,normalize_attn=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3387d64-a9eb-4587-80a5-ba8063505d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def compute_metrics(all_labels,all_preds,num_classes,epoch):\n",
    "    CM = confusion_matrix(all_labels, all_preds, labels=list(range(16)))\n",
    "    acc = np.sum(np.diag(CM)) / np.sum(CM)\n",
    "    \n",
    "    class_sensitivity = []\n",
    "    class_precision = []\n",
    "    class_metrics = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        tp = CM[class_idx, class_idx]\n",
    "        fn = np.sum(CM[class_idx, :]) - tp\n",
    "        fp = np.sum(CM[:, class_idx]) - tp\n",
    "        tn = np.sum(CM) - tp - fn - fp\n",
    "        \n",
    "        sensitivity = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        class_sensitivity.append(sensitivity)\n",
    "        class_precision.append(precision)\n",
    "        class_metrics.append([sensitivity, precision])\n",
    "        \n",
    "    val_mean_sensitivity = np.mean(class_sensitivity)\n",
    "    val_mean_precision = np.mean(class_precision)\n",
    "    return acc,val_mean_sensitivity,val_mean_precision,CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881abdcc-20f6-4c79-a1c5-c833ab4614a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self,patience=5,min_delta=0,restore_best_weigths=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weigths = restore_best_weigths\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self,model,val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model)\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model.load_state_dict(model.state_dict())\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Stopped On {self.counter}\"\n",
    "                if self.restore_best_weigths:\n",
    "                    model.load_state_dict(self.best_model.state_dict())\n",
    "                return True\n",
    "            self.status = f\"{self.counter}/{self.patience}\"\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c64194e-b4dc-4ed5-a057-55842daf2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attn(I, a, up_factor, nrow):\n",
    "    # image\n",
    "    img = I.permute((1,2,0)).cpu().numpy()\n",
    "    # compute the heatmap\n",
    "    if up_factor > 1:\n",
    "        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
    "    attn = utils.make_grid(a, nrow=nrow, normalize=True, scale_each=True)\n",
    "    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()\n",
    "    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)\n",
    "    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)\n",
    "    attn = np.float32(attn) / 255\n",
    "    # add the heatmap to the image\n",
    "    vis = 0.6 * img + 0.4 * attn\n",
    "    return torch.from_numpy(vis).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98dcd8a4-4856-4d7b-934c-07d69e379548",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea6554e5-f694-4ff4-9b9b-32f6594f9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db5c899-f5cb-414f-ac67-ed4f72adb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_lambda = lambda epoch : np.power(0.1, epoch//10)\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7820c569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Traning Loss: 0.5915, Validation Loss:  0.7262, Validation Accuracy,77.95%\n",
      "Epoch [2/30], Traning Loss: 0.5208, Validation Loss:  0.6321, Validation Accuracy,81.21%\n",
      "Epoch [3/30], Traning Loss: 0.4684, Validation Loss:  0.6994, Validation Accuracy,79.52%\n",
      "Epoch [4/30], Traning Loss: 0.4142, Validation Loss:  0.6892, Validation Accuracy,80.14%\n",
      "Epoch [5/30], Traning Loss: 0.2555, Validation Loss:  0.5491, Validation Accuracy,84.16%\n",
      "Epoch [6/30], Traning Loss: 0.2034, Validation Loss:  0.5480, Validation Accuracy,84.28%\n",
      "Epoch [7/30], Traning Loss: 0.1721, Validation Loss:  0.5619, Validation Accuracy,84.32%\n",
      "Epoch [8/30], Traning Loss: 0.1506, Validation Loss:  0.5788, Validation Accuracy,84.05%\n",
      "Epoch [9/30], Traning Loss: 0.1308, Validation Loss:  0.5811, Validation Accuracy,84.39%\n",
      "Epoch [10/30], Traning Loss: 0.1100, Validation Loss:  0.6010, Validation Accuracy,83.82%\n",
      "Epoch [11/30], Traning Loss: 0.0949, Validation Loss:  0.5979, Validation Accuracy,84.28%\n",
      "Epoch [12/30], Traning Loss: 0.0821, Validation Loss:  0.6055, Validation Accuracy,84.36%\n",
      "Epoch [13/30], Traning Loss: 0.0668, Validation Loss:  0.6183, Validation Accuracy,84.05%\n",
      "Epoch [14/30], Traning Loss: 0.0595, Validation Loss:  0.6488, Validation Accuracy,84.28%\n",
      "Epoch [15/30], Traning Loss: 0.0462, Validation Loss:  0.6278, Validation Accuracy,83.82%\n",
      "Epoch [16/30], Traning Loss: 0.0432, Validation Loss:  0.6396, Validation Accuracy,84.28%\n",
      "Epoch [17/30], Traning Loss: 0.0395, Validation Loss:  0.6431, Validation Accuracy,84.09%\n",
      "Epoch [18/30], Traning Loss: 0.0419, Validation Loss:  0.6420, Validation Accuracy,84.36%\n",
      "Epoch [19/30], Traning Loss: 0.0384, Validation Loss:  0.6425, Validation Accuracy,84.20%\n",
      "Epoch [20/30], Traning Loss: 0.0366, Validation Loss:  0.6400, Validation Accuracy,84.09%\n",
      "Epoch [21/30], Traning Loss: 0.0353, Validation Loss:  0.6446, Validation Accuracy,84.28%\n",
      "Epoch [22/30], Traning Loss: 0.0356, Validation Loss:  0.6474, Validation Accuracy,83.82%\n",
      "Epoch [23/30], Traning Loss: 0.0340, Validation Loss:  0.6512, Validation Accuracy,84.32%\n",
      "Epoch [24/30], Traning Loss: 0.0330, Validation Loss:  0.6558, Validation Accuracy,84.20%\n",
      "Epoch [25/30], Traning Loss: 0.0313, Validation Loss:  0.6547, Validation Accuracy,84.05%\n",
      "Epoch [26/30], Traning Loss: 0.0320, Validation Loss:  0.6489, Validation Accuracy,84.16%\n",
      "Epoch [27/30], Traning Loss: 0.0318, Validation Loss:  0.6519, Validation Accuracy,83.90%\n",
      "Epoch [28/30], Traning Loss: 0.0314, Validation Loss:  0.6542, Validation Accuracy,84.36%\n",
      "Epoch [29/30], Traning Loss: 0.0323, Validation Loss:  0.6538, Validation Accuracy,83.97%\n",
      "Epoch [30/30], Traning Loss: 0.0309, Validation Loss:  0.6485, Validation Accuracy,84.28%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        inputs = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs,_,_= model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    writer.add_scalar(\"Training Loss\" , avg_train_loss,epoch)\n",
    "\n",
    "    # Adjusting Learning Rate\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "          inputs = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs,_,_ = model(inputs)\n",
    "          loss = criterion(outputs,labels)\n",
    "          val_loss += loss.item()*images.size(0)\n",
    "          _,predict = torch.max(outputs, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predict == labels).sum().item()\n",
    "          all_preds.extend(predict.cpu().numpy())\n",
    "          all_labels.extend(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy_val = 100*correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Traning Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss: .4f}, Validation Accuracy,{accuracy_val:.2f}%')\n",
    "    \n",
    "    acc,val_mean_sensitivity,val_mean_precision,CM = compute_metrics(all_preds,all_labels,num_classes=16,epoch=epoch)\n",
    "    writer.add_scalar('val/accuracy', acc*100, epoch)\n",
    "    writer.add_scalar('val/mean_recall', val_mean_sensitivity,epoch)\n",
    "    writer.add_scalar('val/precision_mel',val_mean_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Loss\",val_loss,epoch)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    sns.heatmap(CM, annot=True, cmap=\"coolwarm\")\n",
    "\n",
    "    # Add the figure to the SummaryWriter\n",
    "    writer.add_figure(\"heatmap\", fig,global_step=epoch)\n",
    "\n",
    "    # writer.close()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "      best_val_loss = val_loss\n",
    "      checkpoint_path = '/home/rishab/alexnet_attention/saved_model_1'\n",
    "      os.makedirs(checkpoint_path, exist_ok=True)\n",
    "      checkpoint_path = os.path.join(checkpoint_path ,'best_model.pth')\n",
    "      torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    data_iter = iter(val_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    fixed_batch = images[0:16, :, :, :].to(device)\n",
    "    log_images = True\n",
    "    writer = SummaryWriter(\"logs\")\n",
    "    if log_images:\n",
    "        \n",
    "        I_train = utils.make_grid(inputs[0:16, :, :, :], nrow=4, normalize=True, scale_each=True)\n",
    "        writer.add_image('train/image', I_train , global_step = epoch)\n",
    "        \n",
    "    \n",
    "        I_val = utils.make_grid(fixed_batch, nrow=4, normalize=True, scale_each=True)\n",
    "        writer.add_image('val/image', I_val,global_step = epoch)\n",
    "    \n",
    "    base_up_factor = 8\n",
    "    \n",
    "    if log_images:\n",
    "        __, a1, a2 = model(inputs[0:16,:,:,:])\n",
    "        if a1 is not None:\n",
    "            attn1 = visualize_attn(I_train, a1, up_factor=base_up_factor, nrow=4)\n",
    "            writer.add_image('train/attention_map_1', attn1, global_step = epoch)\n",
    "        if a2 is not None:\n",
    "            attn2 = visualize_attn(I_train, a2, up_factor=2*base_up_factor, nrow=4)\n",
    "            writer.add_image('train/attention_map_2', attn2,global_step= epoch)\n",
    "        # val data\n",
    "        __, a1, a2 = model(fixed_batch)\n",
    "        if a1 is not None:\n",
    "            attn1 = visualize_attn(I_val, a1, up_factor=base_up_factor, nrow=4)\n",
    "            writer.add_image('val/attention_map_1', attn1, global_step = epoch)\n",
    "        if a2 is not None:\n",
    "            attn2 = visualize_attn(I_val, a2, up_factor=2*base_up_factor, nrow=4)\n",
    "            writer.add_image('val/attention_map_2', attn2, global_step = epoch) \n",
    "    if epoch == num_epochs - 1:\n",
    "        checkpoint_path = '/home/rishab/alexnet_attention/last_epoch_model'\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_path ,'last_model.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    # if es(model,val_loss):\n",
    "    #     print(\"Early Stopping\")\n",
    "    #     break\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92b8a9-0f78-4e9a-8b3c-d5d30c89b823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
